import numpy as np
import librosa
import sounddevice as sd
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras import backend as K
import pyttsx3
import os
from openai import OpenAI

# ====== é…ç½® DeepSeek API ======
# ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥æ›´å®‰å…¨
client = OpenAI(api_key="sk-f05c168c8cce4590a1ea45a37b4e0583", base_url="https://api.deepseek.com/v1")

# ====== æ‹¼éŸ³æ˜ å°„è¡¨ ======
pinyin_vocab = ['ni', 'hao', 'jin', 'tian', 'qi', 'zen', 'me', 'yang', ' ']
pinyin2idx = {p: i + 1 for i, p in enumerate(pinyin_vocab)}  # 0 ä¸º CTC blank
idx2pinyin = {i + 1: p for i, p in enumerate(pinyin_vocab)}
num_classes = len(pinyin2idx) + 1


# ====== åŠ è½½æ¨ç†æ¨¡å‹ ======
def load_inference_model():
    inference_model = load_model(r'C:\Users\Clodius\Documents\openS\ctc_inference_model.h5')
    return inference_model


# ====== å½•éŸ³ ======
def record_audio(duration=3, sample_rate=16000):
    print("ğŸ¤ å¼€å§‹å½•éŸ³...")
    audio_data = sd.rec(int(duration * sample_rate),
                        samplerate=sample_rate,
                        channels=1,
                        dtype='float32')
    sd.wait()
    print("âœ… å½•éŸ³å®Œæˆã€‚")
    return audio_data.flatten()


# ====== ç‰¹å¾æå–ï¼ˆMFCCï¼‰======
def extract_features(audio_data, sample_rate=16000, n_mfcc=20):
    mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=n_mfcc)
    return np.expand_dims(mfcc.T, axis=0)  # [1, time_steps, feature_dim]


# ====== æ¨¡å‹æ¨ç†ï¼ˆæ‹¼éŸ³è¯†åˆ«ï¼‰======
def predict_with_model(inference_model, X_test):
    y_pred_test = inference_model.predict(X_test)
    input_len = np.ones(y_pred_test.shape[0]) * y_pred_test.shape[1]
    decoded, _ = K.ctc_decode(y_pred_test, input_length=input_len)
    decoded_sequences = decoded[0].numpy()

    result = []
    for seq in decoded_sequences:
        result = [idx2pinyin.get(i, '?') for i in seq if i > 0]
    return result


# ====== è°ƒç”¨ DeepSeek æ¥å£è·å–å›ç­” ======
def ask_deepseek(question):
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",  # DeepSeekä¸“ç”¨æ¨¡å‹åç§°
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”èªæ˜çš„è¯­éŸ³åŠ©æ‰‹ï¼Œä¸­æ–‡åå­—å«å°å¨œã€‚"},
                {"role": "user", "content": question}
            ],
            temperature=0.2  # æ§åˆ¶å›ç­”éšæœºæ€§
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"


# ====== è¯­éŸ³æ’­æŠ¥ ======
def text_to_speech(text):
    engine = pyttsx3.init()
    # ä¸­æ–‡è¯­éŸ³ä¼˜åŒ–è®¾ç½®
    engine.setProperty('rate', 180)  # è¯­é€Ÿ
    engine.setProperty('volume', 0.9)  # éŸ³é‡
    engine.setProperty('voice', 'zh')  # ä¸­æ–‡è¯­éŸ³
    engine.say(text)
    engine.runAndWait()


# ====== ä¸»ç¨‹åºå…¥å£ ======
if __name__ == "__main__":
    print("=== ä¸­æ–‡è¯­éŸ³åŠ©æ‰‹(DeepSeekç‰ˆ) ===")

    # 1. åŠ è½½æ¨¡å‹
    try:
        inference_model = load_inference_model()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        exit()

    # 2. å½•éŸ³
    try:
        audio_data = record_audio(duration=3)
        X_test = extract_features(audio_data)
    except Exception as e:
        print(f"âŒ å½•éŸ³å¤±è´¥: {e}")
        exit()

    # 3. è¯†åˆ«æ‹¼éŸ³
    try:
        result = predict_with_model(inference_model, X_test)
        print("ğŸ—£ï¸ è¯†åˆ«ç»“æœ:", " ".join(result))
    except Exception as e:
        print(f"âŒ è¯†åˆ«å¤±è´¥: {e}")
        exit()

    # 4. ä¸DeepSeekäº¤äº’
    if not result:
        print("âš ï¸ æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹")
    else:
        query = "".join(result)
        try:
            response = ask_deepseek(query)
            print("ğŸ¤– åŠ©æ‰‹å›å¤:", response)
            text_to_speech(response)
        except Exception as e:
            print(f"âŒ äº¤äº’å¤±è´¥: {e}")